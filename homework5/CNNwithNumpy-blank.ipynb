{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于python基础库，搭建卷积神经网络，并用该网络进行手写字符识别\n",
    "\n",
    "#### 本程序主要基于python的numpy、math等基础函数库，完成了CNN训练的前向传播、后向传播、随机梯度下降更新等主要的函数功能；\n",
    "\n",
    "#### 并基于该程序，在MNIST数据集上进行手写字符识别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import,division,print_function\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "from six.moves import xrange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#包推荐版本见requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一些超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE=28           #图片大小\n",
    "NUM_CHANNELS=1          #输入图片通道数\n",
    "PIXEL_DEPTH=255         #灰度\n",
    "BATCH_SIZE=4           #训练时的batch size\n",
    "NUM_LABELS=10           #类别数量\n",
    "MAX_EPOCHS = 1          #训练epochs\n",
    "SEED=2330               #random seed\n",
    "EVAL_FREQUENCY = 100    #验证频率\n",
    "EVAL_BATCH_SIZE = 4    #验证时的batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入MNIST训练数据集，并将训练集划分出验证集\n",
    "\n",
    "具体函数请参看`data_helper.py`文件，已实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data,  ./data/train-images-idx3-ubyte.gz\n",
      "Extracting labels, ./data/train-labels-idx1-ubyte.gz\n",
      "Extracting data,  ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting labels, ./data/t10k-labels-idx1-ubyte.gz\n",
      "train data size:  55000\n",
      "train data shape:  (55000, 28, 28, 1)\n",
      "train label shape:  (55000, 10)\n",
      "test data shape:  (10000, 28, 28, 1)\n",
      "test label shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from data_helper import load_train,load_test\n",
    "\n",
    "def onehot(targets, num):\n",
    "    result = np.zeros((num, 10))\n",
    "    for i in range(num):\n",
    "        result[i][targets[i]] = 1\n",
    "    return result\n",
    "\n",
    "validation_size = 5000\n",
    "train_data,train_labels,validation_data,validation_labels = load_train(validation_size,IMAGE_SIZE, NUM_CHANNELS, PIXEL_DEPTH)\n",
    "test_data,test_labels = load_test(IMAGE_SIZE, NUM_CHANNELS, PIXEL_DEPTH)\n",
    "\n",
    "train_labels = onehot(train_labels, 55000)\n",
    "validation_labels = onehot(validation_labels, validation_size)\n",
    "train_size = train_labels.shape[0]\n",
    "print(\"train data size: \",len(train_data))\n",
    "print(\"train data shape: \",train_data.shape)\n",
    "print(\"train label shape: \",train_labels.shape)\n",
    "print(\"test data shape: \", test_data.shape)\n",
    "print(\"test label shape: \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图片示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 1\n",
    "plt.imshow(train_data[index].reshape((28,28)),cmap = plt.cm.gray)\n",
    "print(\"y is: \"+str(train_labels[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 卷积操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: \t (2, 28, 28, 1)\n",
      "conv_out.shape:  (2, 26, 26, 2)\n",
      "conv_back.shape: (2, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, layer_shape, k_size=5, k_num=32, strides=1, seed=2330):\n",
    "        #输入的维度 （batch，height，width，channels）\n",
    "        self.input_shape = layer_shape\n",
    "        self.input_batch = layer_shape[0]\n",
    "        self.input_height = layer_shape[1]\n",
    "        self.input_width = layer_shape[2]\n",
    "        self.input_channels = layer_shape[3]\n",
    "        \n",
    "        self.seed = seed\n",
    "        self.k_size = k_size      #kernel大小\n",
    "        self.strides = strides    #kernel步长\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        #stride和图片大小是否对应\n",
    "        if (self.input_height-self.k_size) % self.strides != 0:\n",
    "            print(\"input tensor height can\\'t fit strides!\")\n",
    "        if (self.input_width-self.k_size) % self.strides != 0:\n",
    "            print(\"input tensor width can\\'t fit strides!\")\n",
    "\n",
    "        #输出的维度\n",
    "        self.output_batch = self.input_batch\n",
    "        self.output_height = (self.input_height-self.k_size) // self.strides + 1\n",
    "        self.output_width = (self.input_width-self.k_size) // self.strides + 1\n",
    "        self.output_channels = k_num\n",
    "        self.delta = np.zeros((self.output_batch, self.output_height, self.output_width, self.output_channels))\n",
    "        self.col_image = np.zeros((self.output_batch, self.output_height*self.output_width, self.k_size*self.k_size*self.input_channels))\n",
    "        self.output_shape = self.delta.shape\n",
    "        \n",
    "        weights_scale = math.sqrt(k_size*k_size*self.output_channels/2)\n",
    "        #卷积权重初始化\n",
    "        self.weights = np.random.standard_normal((k_size, k_size, self.input_channels, self.output_channels)) / weights_scale\n",
    "        self.bias = np.random.standard_normal(self.output_channels) / weights_scale\n",
    "        #梯度初始化\n",
    "        self.w_gradient = np.zeros(self.weights.shape)\n",
    "        self.b_gradient = np.zeros(self.bias.shape)\n",
    "    \n",
    "    def img2col(self, image, ksize, stride):\n",
    "        # image([batchsize, width ,height, channel])\n",
    "        #取出所有要和kernel做卷积的图像区域\n",
    "        image_col = []\n",
    "        for i in range(0, image.shape[1] - ksize + 1, stride):\n",
    "            for j in range(0, image.shape[2] - ksize + 1, stride):\n",
    "                col = image[:, i:i + ksize, j:j + ksize, :].reshape([-1])\n",
    "                image_col.append(col)\n",
    "        image_col = np.array(image_col)\n",
    "\n",
    "        return image_col\n",
    "        \n",
    "    #计算卷积的前向传播和部分反向求导的参数更新（20 points）\n",
    "    def forward(self,X):\n",
    "        ### start your code\n",
    "        # tips: 对一个batch，逐图片找其要计算的所有区域，计算卷积\n",
    "        conv_out = np.zeros(self.delta.shape)\n",
    "        for batch in range(self.input_batch):\n",
    "            cur_img = X[batch][np.newaxis,:]\n",
    "            batch2col = self.img2col(cur_img, self.k_size, self.strides)\n",
    "            self.col_image[batch] = batch2col\n",
    "            conv_out[batch] = np.reshape(np.dot(batch2col,self.weights.reshape([self.k_size*self.k_size*self.input_channels,-1]))+self.bias,[self.output_height,self.output_width,-1])\n",
    "        ###end your code\n",
    "        return conv_out\n",
    "        \n",
    "    \n",
    "    def backward(self, delta, lr=0.0001, weight_decay=0.0004):\n",
    "        self.delta = delta\n",
    "        col_delta = np.reshape(delta, [self.input_batch, -1, self.output_channels])\n",
    "    \n",
    "        for i in range(self.input_batch):\n",
    "            self.w_gradient += np.dot(self.col_image[i].T, col_delta[i]).reshape(self.weights.shape)\n",
    "        self.b_gradient += np.sum(col_delta, axis=(0,1))\n",
    "        \n",
    "\n",
    "        pad_delta = np.pad(self.delta, \n",
    "                            ((0, 0), (self.k_size - 1, self.k_size - 1), (self.k_size - 1, self.k_size - 1), (0, 0)),\n",
    "                            'constant', constant_values=0)\n",
    "\n",
    "        flip_weights = np.flipud(np.fliplr(self.weights))\n",
    "        flip_weights = flip_weights.swapaxes(2, 3)\n",
    "        col_flip_weights = flip_weights.reshape([-1, self.input_channels])\n",
    "        col_pad_delta = np.array([self.img2col(pad_delta[i][np.newaxis, :], self.k_size, self.strides) for i in range(self.input_batch)])\n",
    "        delta_back = np.dot(col_pad_delta, col_flip_weights)\n",
    "        delta_back = np.reshape(delta_back, self.input_shape)\n",
    "        \n",
    "        #参数更新\n",
    "        # update weights，bias，清空w_gradient, b_gradient\n",
    "        ### start your code\n",
    "        # tips: weight decay使用：(1-weight_decay)*(weight或者bias)-lr*(w_gradient或者b_gradient),作为一个权重使用\n",
    "        self.weights = (1 - weight_decay) * self.weights - lr * self.w_gradient\n",
    "        self.bias = (1 - weight_decay) * self.bias - lr * self.b_gradient\n",
    "        self.w_gradient = np.zeros(self.weights.shape)\n",
    "        self.b_gradient = np.zeros(self.bias.shape)\n",
    "        ###end your code\n",
    "        return delta_back\n",
    "\n",
    "def test_():\n",
    "    index = 1\n",
    "    img = train_data[index:index+2, ...]\n",
    "    y = train_labels[index:index+2,...]\n",
    "    print(\"img.shape: \\t\", img.shape)\n",
    "    \n",
    "    conv = Convolution(img.shape, k_size=3, k_num=2, strides=1, seed=2330)\n",
    "    conv_out = conv.forward(img)\n",
    "    print(\"conv_out.shape: \", conv_out.shape)\n",
    "\n",
    "    \n",
    "    conv_back = conv.backward(conv_out, lr=0.0001)\n",
    "    print(\"conv_back.shape:\", conv_back.shape)\n",
    "    \n",
    "test_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expect outputs:\n",
    "    \n",
    "    img.shape:       (2, 28, 28, 1)\n",
    "    conv_out.shape:  (2, 26, 26, 2)\n",
    "    conv_back.shape: (2, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ReLu 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: \t (2, 28, 28, 1)\n",
      "conv_out.shape:  (2, 26, 26, 2)\n",
      "relu_out.shape:  (2, 26, 26, 2)\n",
      "relu_back.shape: (2, 26, 26, 2)\n",
      "conv_back.shape: (2, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# rule Activator\n",
    "class Relu:  \n",
    "    def __init__(self, input_shape):\n",
    "        self.delta = np.zeros(input_shape)\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = self.input_shape\n",
    "    \n",
    "    #根据relu函数的形式，写出其前向传播与反向求导的值 （10 points）\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        ###start your code\n",
    "        result=np.maximum(0, x)\n",
    "        ###end your code\n",
    "        return result\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        ###start your code\n",
    "        delta[self.x < 0] = 0\n",
    "        ###end your code\n",
    "        return delta\n",
    "    \n",
    "def test_():\n",
    "    index = 1\n",
    "    img = train_data[index:index+2, ...]\n",
    "    y = train_labels[index:index+2,...]\n",
    "    print(\"img.shape: \\t\", img.shape)\n",
    "    \n",
    "    conv = Convolution(img.shape, k_size=3, k_num=2, strides=1, seed=2330)\n",
    "    conv_out = conv.forward(img)\n",
    "    print(\"conv_out.shape: \", conv_out.shape)\n",
    "\n",
    "    relu=Relu(conv_out.shape)\n",
    "    relu_out = relu.forward(conv_out)\n",
    "    print(\"relu_out.shape: \", relu_out.shape)\n",
    "    \n",
    "    relu_back = relu.backward(relu_out)\n",
    "    print(\"relu_back.shape:\", relu_back.shape)\n",
    "    \n",
    "    conv_back = conv.backward(relu_back, lr=0.0001)\n",
    "    print(\"conv_back.shape:\", conv_back.shape)\n",
    "    \n",
    "test_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expect output:\n",
    "    \n",
    "    img.shape: \t  (2, 28, 28, 1)\n",
    "    conv_out.shape:  (2, 26, 26, 2)\n",
    "    relu_out.shape:  (2, 26, 26, 2)\n",
    "    relu_back.shape: (2, 26, 26, 2)\n",
    "    conv_back.shape: (2, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. max_pooling 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: \t (2, 28, 28, 1)\n",
      "conv_out.shape:  (2, 26, 26, 2)\n",
      "relu_out.shape:  (2, 26, 26, 2)\n",
      "pool_out.shape:  (2, 13, 13, 2)\n",
      "pool_back.shape: (2, 26, 26, 2)\n",
      "relu_back.shape: (2, 26, 26, 2)\n",
      "conv_back.shape: (2, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "class max_pool:\n",
    "    def __init__(self, input_shape, k_size=2, strides=2):\n",
    "        self.input_shape = input_shape\n",
    "        self.k_size = k_size\n",
    "        self.strides = strides\n",
    "        self.output_shape = [input_shape[0], input_shape[1] // self.strides, input_shape[2] // self.strides, input_shape[3]]\n",
    "        self.feature_mask = np.zeros(input_shape)\n",
    "    \n",
    "    #计算最大池化的前向传播和反向求导(20 points)\n",
    "    def forward(self, x):\n",
    "        ### start your code\n",
    "        # tips：记录最大池化时最大值的位置信息设为类的变量self.feature_mask，用于反向传播\n",
    "        #       活用np.argmax, 切片等操作\n",
    "        feature = np.zeros(self.output_shape)\n",
    "        for b in range(self.input_shape[0]):# each batch\n",
    "            for c in range(self.input_shape[3]):# each channel\n",
    "                for i in range(0, self.input_shape[1], self.strides):\n",
    "                    for j in range(0, self.input_shape[2], self.strides):\n",
    "                        pool_field = x[b, i:i+self.k_size, j:j+self.k_size, c]\n",
    "                        max_index = np.argmax(pool_field)\n",
    "                        feature[b, i//self.strides, j//self.strides, c] = pool_field.flatten()[max_index]\n",
    "                        self.feature_mask[b, i+max_index//self.k_size, j+max_index%self.k_size, c] = 1\n",
    "        ### end your code\n",
    "        return feature\n",
    "\n",
    "    def backward(self, delta):\n",
    "        ### start your code\n",
    "        delta_back = np.zeros(self.input_shape)\n",
    "        for b in range(delta.shape[0]):# each batch\n",
    "            for c in range(delta.shape[3]):# each channel\n",
    "                for i in range(delta.shape[1]):\n",
    "                    for j in range(delta.shape[2]):\n",
    "                        delta_back[b, i*self.strides:(i+1)*self.strides, j*self.strides:(j+1)*self.strides, c] = delta[b,i,j,c] * self.feature_mask[b,i*self.strides:(i+1)*self.strides,j*self.strides:(j+1)*self.strides,c]\n",
    "        ### end your code\n",
    "        return delta_back\n",
    "    \n",
    "def test_():\n",
    "    index = 1\n",
    "    img = train_data[index:index+2, ...]\n",
    "    y = train_labels[index:index+2,...]\n",
    "    print(\"img.shape: \\t\", img.shape)\n",
    "    \n",
    "    conv = Convolution(img.shape, k_size=3, k_num=2, strides=1, seed=2330)\n",
    "    conv_out = conv.forward(img)\n",
    "    print(\"conv_out.shape: \", conv_out.shape)\n",
    "\n",
    "    relu=Relu(conv_out.shape)\n",
    "    relu_out = relu.forward(conv_out)\n",
    "    print(\"relu_out.shape: \", relu_out.shape)\n",
    "    \n",
    "    pool = max_pool(relu_out.shape,2,2)\n",
    "    pool_out = pool.forward(relu_out)\n",
    "    print(\"pool_out.shape: \", pool_out.shape)\n",
    "    \n",
    "    pool_back = pool.backward(pool_out)\n",
    "    print(\"pool_back.shape:\", pool_back.shape)\n",
    "    \n",
    "    relu_back = relu.backward(pool_back)\n",
    "    print(\"relu_back.shape:\", relu_back.shape)\n",
    "    \n",
    "    conv_back = conv.backward(relu_back, lr=0.0001)\n",
    "    print(\"conv_back.shape:\", conv_back.shape)\n",
    "    \n",
    "test_()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expect output:\n",
    "    \n",
    "    img.shape: \t  (2, 28, 28, 1)\n",
    "    conv_out.shape:  (2, 26, 26, 2)\n",
    "    relu_out.shape:  (2, 26, 26, 2)\n",
    "    pool_out.shape:  (2, 13, 13, 2)\n",
    "    pool_back.shape: (2, 26, 26, 2)\n",
    "    relu_back.shape: (2, 26, 26, 2)\n",
    "    conv_back.shape: (2, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: \t (2, 28, 28, 1)\n",
      "conv_out.shape:  (2, 26, 26, 2)\n",
      "relu_out.shape:  (2, 26, 26, 2)\n",
      "pool_out.shape:  (2, 13, 13, 2)\n",
      "flat_out.shape:  (2, 338)\n",
      "flat_back.shape: (2, 13, 13, 2)\n",
      "pool_back.shape: (2, 26, 26, 2)\n",
      "relu_back.shape: (2, 26, 26, 2)\n",
      "conv_back.shape: (2, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "class flatten:\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = [self.input_shape[0], self.input_shape[1] * self.input_shape[2] * self.input_shape[3]]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x.reshape([self.input_shape[0], self.input_shape[1] * self.input_shape[2] * self.input_shape[3]])\n",
    "        return y\n",
    "    def backward(self, y):\n",
    "        x = np.reshape(y, [self.input_shape[0], self.input_shape[1], self.input_shape[2], self.input_shape[3]])\n",
    "        return x\n",
    "\n",
    "def test_():\n",
    "    index = 1\n",
    "    img = train_data[index:index+2, ...]\n",
    "    y = train_labels[index:index+2,...]\n",
    "    print(\"img.shape: \\t\", img.shape)\n",
    "    \n",
    "    conv = Convolution(img.shape, k_size=3, k_num=2, strides=1, seed=2330)\n",
    "    conv_out = conv.forward(img)\n",
    "    print(\"conv_out.shape: \", conv_out.shape)\n",
    "\n",
    "    relu=Relu(conv.output_shape)\n",
    "    relu_out = relu.forward(conv_out)\n",
    "    print(\"relu_out.shape: \", relu_out.shape)\n",
    "    \n",
    "    pool = max_pool(relu.output_shape,2,2)\n",
    "    pool_out = pool.forward(relu_out)\n",
    "    print(\"pool_out.shape: \", pool_out.shape)\n",
    "    \n",
    "    flat = flatten(pool.output_shape)\n",
    "    flat_out = flat.forward(pool_out)\n",
    "    print(\"flat_out.shape: \", flat_out.shape)\n",
    "    \n",
    "    flat_back = flat.backward(flat_out)\n",
    "    print(\"flat_back.shape:\", flat_back.shape)\n",
    "    \n",
    "    pool_back = pool.backward(flat_back)\n",
    "    print(\"pool_back.shape:\", pool_back.shape)\n",
    "    \n",
    "    relu_back = relu.backward(pool_back)\n",
    "    print(\"relu_back.shape:\", relu_back.shape)\n",
    "    \n",
    "    conv_back = conv.backward(relu_back, lr=0.0001)\n",
    "    print(\"conv_back.shape:\", conv_back.shape)\n",
    "    \n",
    "test_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 全连接层\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: \t (2, 28, 28, 1)\n",
      "conv_out.shape:  (2, 26, 26, 2)\n",
      "relu_out.shape:  (2, 26, 26, 2)\n",
      "pool_out.shape:  (2, 13, 13, 2)\n",
      "flat_out.shape:  (2, 338)\n",
      "fc_out.shape: \t (2, 10)\n",
      "fc_back.shape: \t (2, 338)\n",
      "flat_back.shape: (2, 13, 13, 2)\n",
      "pool_back.shape: (2, 26, 26, 2)\n",
      "relu_back.shape: (2, 26, 26, 2)\n",
      "conv_back.shape: (2, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "class full_connection:\n",
    "    def __init__(self, input_shape, output_channels, seed=2330):\n",
    "        self.input_shape = input_shape\n",
    "        self.input_batch = input_shape[0]\n",
    "        self.input_length = input_shape[1]\n",
    "        self.output_channels = output_channels\n",
    "        \n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        weights_scale = math.sqrt(self.input_length/2) \n",
    "        self.weights = np.random.standard_normal((self.input_length,self.output_channels)) / weights_scale\n",
    "        self.bias = np.random.standard_normal(self.output_channels) / weights_scale\n",
    "        \n",
    "        self.output_shape = [self.input_batch, self.output_channels]\n",
    "        self.w_gradient = np.zeros(self.weights.shape)\n",
    "        self.b_gradient = np.zeros(self.bias.shape)\n",
    "        \n",
    "        self.x = np.zeros(input_shape)\n",
    "    \n",
    "    #计算全连接的前向传播，反向求导 (20 points)\n",
    "    def forward(self, x):\n",
    "        ### start your code\n",
    "        # tips: wx+b形式\n",
    "        self.x = x\n",
    "        y = np.dot(x, self.weights) + self.bias\n",
    "        ### end your code\n",
    "        return y\n",
    "    \n",
    "    def backward(self, delta, lr=0.0001, weight_decay=0.0004):\n",
    "        ### start your code\n",
    "        # tips: weight decay使用：(1-weightdecay)*weight-lr*w_gradient, bias类似\n",
    "        self.w_gradient = np.dot(self.x.T, delta)\n",
    "        self.b_gradient = np.sum(delta, axis=0)\n",
    "        delta_back = np.dot(delta, self.weights.T)\n",
    "        \n",
    "        self.weights = (1 - weight_decay) * self.weights - lr * self.w_gradient\n",
    "        self.bias = (1 - weight_decay) * self.bias - lr * self.b_gradient\n",
    "        ### end your code\n",
    "        return delta_back\n",
    "    \n",
    "def test_():\n",
    "    index = 1\n",
    "    img = train_data[index:index+2, ...]\n",
    "    y = train_labels[index:index+2,...]\n",
    "    print(\"img.shape: \\t\", img.shape)\n",
    "    \n",
    "    conv = Convolution(img.shape, k_size=3, k_num=2, strides=1, seed=2330)\n",
    "    conv_out = conv.forward(img)\n",
    "    print(\"conv_out.shape: \", conv_out.shape)\n",
    "\n",
    "    relu=Relu(conv.output_shape)\n",
    "    relu_out = relu.forward(conv_out)\n",
    "    print(\"relu_out.shape: \", relu_out.shape)\n",
    "    \n",
    "    pool = max_pool(relu.output_shape,2,2)\n",
    "    pool_out = pool.forward(relu_out)\n",
    "    print(\"pool_out.shape: \", pool_out.shape)\n",
    "    \n",
    "    flat = flatten(pool.output_shape)\n",
    "    flat_out = flat.forward(pool_out)\n",
    "    print(\"flat_out.shape: \", flat_out.shape)\n",
    "    \n",
    "    fc = full_connection(flat.output_shape,10,seed=SEED)\n",
    "    fc_out = fc.forward(flat_out)\n",
    "    print(\"fc_out.shape: \\t\", fc_out.shape)\n",
    "    \n",
    "    fc_back = fc.backward(fc_out,lr=0.0001)\n",
    "    print(\"fc_back.shape: \\t\",fc_back.shape)\n",
    "    \n",
    "    flat_back = flat.backward(fc_back)\n",
    "    print(\"flat_back.shape:\", flat_back.shape)\n",
    "    \n",
    "    pool_back = pool.backward(flat_back)\n",
    "    print(\"pool_back.shape:\", pool_back.shape)\n",
    "    \n",
    "    relu_back = relu.backward(pool_back)\n",
    "    print(\"relu_back.shape:\", relu_back.shape)\n",
    "    \n",
    "    conv_back = conv.backward(relu_back, lr=0.0001)\n",
    "    print(\"conv_back.shape:\", conv_back.shape)\n",
    "    \n",
    "test_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expect output:\n",
    "\n",
    "    img.shape: \t  (2, 28, 28, 1)\n",
    "    conv_out.shape:  (2, 28, 28, 2)\n",
    "    relu_out.shape:  (2, 28, 28, 2)\n",
    "    pool_out.shape:  (2, 14, 14, 2)\n",
    "    flat_out.shape:  (2, 392)\n",
    "    fc_out.shape:    (2, 10)\n",
    "    fc_back.shape:   (2, 392)\n",
    "    flat_back.shape: (2, 14, 14, 2)\n",
    "    pool_back.shape: (2, 28, 28, 2)\n",
    "    relu_back.shape: (2, 28, 28, 2)\n",
    "    conv_back.shape: (2, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Softmax 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: \t (2, 28, 28, 1)\n",
      "conv_out.shape:  (2, 24, 24, 32)\n",
      "relu_out.shape:  (2, 24, 24, 32)\n",
      "pool_out.shape:  (2, 12, 12, 32)\n",
      "flat_out.shape:  (2, 4608)\n",
      "fc_out.shape: \t (2, 10)\n",
      "pred.shape: \t (2, 10)\n",
      "loss:  \t\t 2.2156761458064804\n",
      "loss_back.shape: (2, 10)\n",
      "fc_back.shape: \t (2, 4608)\n",
      "flat_back.shape: (2, 12, 12, 32)\n",
      "pool_back.shape: (2, 24, 24, 32)\n",
      "relu_back.shape: (2, 24, 24, 32)\n",
      "conv_back.shape: (2, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "class Softmax:\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.input_batch = input_shape[0]\n",
    "        self.class_num = input_shape[1]\n",
    "        \n",
    "        self.softmax = np.zeros(self.input_shape)\n",
    "        self.delta = np.zeros(self.input_shape)\n",
    "        \n",
    "    def cal_loss(self, x, label):\n",
    "        self.prediction(x)\n",
    "        loss = 0\n",
    "        for i in range(self.input_batch):\n",
    "            loss -= np.sum(np.log(self.softmax[i]) * label[i])\n",
    "        loss /= self.input_batch\n",
    "        return loss\n",
    "    \n",
    "    #计算softmax的前向传播，反向求导（10 points）\n",
    "    def prediction(self, x):\n",
    "        for i in range(self.input_batch):\n",
    "            ### start your code\n",
    "            self.softmax[i] = np.exp(x[i]) / np.sum(np.exp(x[i]))\n",
    "            ## end your code\n",
    "        return self.softmax\n",
    "    \n",
    "    def backward(self, label):\n",
    "        for i in range(self.input_batch):\n",
    "            ### start your code\n",
    "            self.delta[i] = self.softmax[i] - label[i]\n",
    "            ## end your code\n",
    "        return self.delta\n",
    "    \n",
    "def test_():\n",
    "    index = 1\n",
    "    img = train_data[index:index+2, ...]\n",
    "    y = train_labels[index:index+2,...]\n",
    "    print(\"img.shape: \\t\", img.shape)\n",
    "    \n",
    "    conv = Convolution(img.shape, k_size=5, k_num=32, strides=1, seed=2330)\n",
    "    conv_out = conv.forward(img)\n",
    "    print(\"conv_out.shape: \", conv_out.shape)\n",
    "\n",
    "    relu=Relu(conv.output_shape)\n",
    "    relu_out = relu.forward(conv_out)\n",
    "    print(\"relu_out.shape: \", relu_out.shape)\n",
    "    \n",
    "    pool = max_pool(relu.output_shape,2,2)\n",
    "    pool_out = pool.forward(relu_out)\n",
    "    print(\"pool_out.shape: \", pool_out.shape)\n",
    "    \n",
    "    flat = flatten(pool.output_shape)\n",
    "    flat_out = flat.forward(pool_out)\n",
    "    print(\"flat_out.shape: \", flat_out.shape)\n",
    "    \n",
    "    fc = full_connection(flat.output_shape,10,seed=SEED)\n",
    "    fc_out = fc.forward(flat_out)\n",
    "    print(\"fc_out.shape: \\t\", fc_out.shape)\n",
    "    \n",
    "    softmax = Softmax(fc.output_shape)\n",
    "    \n",
    "    pred = softmax.prediction(fc_out)\n",
    "    print(\"pred.shape: \\t\", pred.shape)\n",
    "    \n",
    "    loss = softmax.cal_loss(fc_out, y)\n",
    "    print(\"loss:  \\t\\t\", loss)\n",
    "    \n",
    "    loss_back = softmax.backward(label=y)\n",
    "    print(\"loss_back.shape:\", loss_back.shape)\n",
    "    \n",
    "    fc_back = fc.backward(loss_back,lr=0.0001)\n",
    "    print(\"fc_back.shape: \\t\",fc_back.shape)\n",
    "    \n",
    "    flat_back = flat.backward(fc_back)\n",
    "    print(\"flat_back.shape:\", flat_back.shape)\n",
    "    \n",
    "    pool_back = pool.backward(flat_back)\n",
    "    print(\"pool_back.shape:\", pool_back.shape)\n",
    "    \n",
    "    relu_back = relu.backward(pool_back)\n",
    "    print(\"relu_back.shape:\", relu_back.shape)\n",
    "    \n",
    "    conv_back = conv.backward(relu_back, lr=0.0001)\n",
    "    print(\"conv_back.shape:\", conv_back.shape)\n",
    "    \n",
    "    \n",
    "test_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expect output:\n",
    "    \n",
    "    img.shape: \t  (2, 28, 28, 1)\n",
    "    conv_out.shape:  (2, 24, 24, 32)\n",
    "    relu_out.shape:  (2, 24, 24, 32)\n",
    "    pool_out.shape:  (2, 12, 12, 32)\n",
    "    flat_out.shape:  (2, 4608)\n",
    "    fc_out.shape:    (2, 10)\n",
    "    pred.shape: \t (2, 10)\n",
    "    loss:  \t\t   2.32...\n",
    "    loss_back.shape: (2, 10)\n",
    "    fc_back.shape:   (2, 4608)\n",
    "    flat_back.shape: (2, 12, 12, 32)\n",
    "    pool_back.shape: (2, 24, 24, 32)\n",
    "    relu_back.shape: (2, 24, 24, 32)\n",
    "    conv_back.shape: (2, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 搭建CNN网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self,num_labels=10, batch_size=64, image_size=28, num_channels=1, seed=66478):\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.num_channels = num_channels\n",
    "        self.seed = seed\n",
    "        self.num_labels=num_labels\n",
    "        self.net_builder()\n",
    "        \n",
    "    def net_builder(self):\n",
    "        self.conv1 = Convolution([self.batch_size, self.image_size, self.image_size, self.num_channels], k_size=5, k_num=6, strides=1, seed=2330)\n",
    "        self.relu1 = Relu(self.conv1.output_shape)\n",
    "        self.pool1 = max_pool(self.relu1.output_shape, 2,2)\n",
    "            \n",
    "        self.conv2 = Convolution(self.pool1.output_shape, k_size=5, k_num=16, strides=1, seed=2330)\n",
    "        self.relu2 = Relu(self.conv2.output_shape)\n",
    "        self.pool2 = max_pool(self.relu2.output_shape, 2,2)\n",
    "            \n",
    "        self.flat = flatten(self.pool2.output_shape)\n",
    "        self.fc1 = full_connection(self.flat.output_shape,512,seed=SEED)\n",
    "        self.relu3 = Relu(self.fc1.output_shape)\n",
    "            \n",
    "        self.fc2 = full_connection(self.relu3.output_shape,10,seed=SEED)\n",
    "            \n",
    "        self.softmax = Softmax(self.fc2.output_shape)\n",
    "        \n",
    "    def cal_forward(self,x):\n",
    "        conv1_out = self.conv1.forward(x)\n",
    "        relu1_out = self.relu1.forward(conv1_out)\n",
    "        pool1_out = self.pool1.forward(relu1_out)\n",
    "        \n",
    "        conv2_out = self.conv2.forward(pool1_out)\n",
    "        relu2_out = self.relu2.forward(conv2_out)\n",
    "        pool2_out = self.pool2.forward(relu2_out)\n",
    "        \n",
    "        flat_out = self.flat.forward(pool2_out)\n",
    "        fc1_out = self.fc1.forward(flat_out)\n",
    "        relu3_out = self.relu3.forward(fc1_out)\n",
    "        \n",
    "        fc2_out = self.fc2.forward(relu3_out)\n",
    "\n",
    "        pred = self.softmax.prediction(fc2_out)\n",
    "        return np.argmax(pred,axis=1)\n",
    "\n",
    "    def fit(self, x, y, lr):\n",
    "        ### start your code （20 points）\n",
    "        # 包括前向传播（和上面forward形式一致），到计算pred\n",
    "        # 在已知forward的结构条件下，反向逐步计算梯度，从计算loss开始\n",
    "        # tips: 都是用前面构建的函数搭建，直接调用backward即可，重点在于求导顺序\n",
    "        \n",
    "        # forward\n",
    "        conv1_out = self.conv1.forward(x)\n",
    "        relu1_out = self.relu1.forward(conv1_out)\n",
    "        pool1_out = self.pool1.forward(relu1_out)\n",
    "        \n",
    "        conv2_out = self.conv2.forward(pool1_out)\n",
    "        relu2_out = self.relu2.forward(conv2_out)\n",
    "        pool2_out = self.pool2.forward(relu2_out)\n",
    "        \n",
    "        flat_out = self.flat.forward(pool2_out)\n",
    "        fc1_out = self.fc1.forward(flat_out)\n",
    "        relu3_out = self.relu3.forward(fc1_out)\n",
    "        \n",
    "        fc2_out = self.fc2.forward(relu3_out)\n",
    "\n",
    "        pred = self.softmax.prediction(fc2_out)\n",
    "        \n",
    "        loss = self.softmax.cal_loss(fc2_out, y)\n",
    "        \n",
    "        # backward\n",
    "        delta = self.softmax.backward(y)\n",
    "        \n",
    "        delta = self.fc2.backward(delta, lr=lr)\n",
    "        \n",
    "        delta = self.relu3.backward(delta)\n",
    "        delta = self.fc1.backward(delta, lr=lr)\n",
    "        delta = self.flat.backward(delta)\n",
    "        \n",
    "        delta = self.pool2.backward(delta)\n",
    "        delta = self.relu2.backward(delta)\n",
    "        delta = self.conv2.backward(delta, lr=lr)\n",
    "\n",
    "        delta = self.pool1.backward(delta)\n",
    "        delta = self.relu1.backward(delta)\n",
    "        delta = self.conv1.backward(delta, lr=lr)\n",
    "        \n",
    "        #end your code\n",
    "        return loss, np.argmax(pred,axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy\n",
    "def cal_acc(predictions, labels):\n",
    "    return (100.0 * np.sum(predictions == labels) /predictions.shape[0]) \n",
    "\n",
    "def eval_in_batches(data,cnn):\n",
    "    size = data.shape[0]\n",
    "    if size < EVAL_BATCH_SIZE:\n",
    "        raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n",
    "    predictions = np.zeros(size)\n",
    "    for begin in xrange(0, size, EVAL_BATCH_SIZE):\n",
    "        end = begin + EVAL_BATCH_SIZE\n",
    "        if end <= size:\n",
    "            x = data[begin:end, ...]\n",
    "            predictions[begin:end] = cnn.cal_forward(x)\n",
    "        else:\n",
    "            x = data[-EVAL_BATCH_SIZE:, ...]\n",
    "            batch_predictions = cnn.cal_forward(x)\n",
    "            predictions[begin:] = batch_predictions[begin - size:]\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (epoch 0.00)\n",
      "Minibatch loss: 2.426\n",
      "Minibatch acc:  0.0%\n",
      "Validation error: 15.0%\n",
      "Step 100 (epoch 0.01)\n",
      "Minibatch loss: 0.827\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 77.3%\n",
      "Step 200 (epoch 0.01)\n",
      "Minibatch loss: 0.690\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 77.8%\n",
      "Step 300 (epoch 0.02)\n",
      "Minibatch loss: 0.277\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 90.4%\n",
      "Step 400 (epoch 0.03)\n",
      "Minibatch loss: 0.412\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 89.8%\n",
      "Step 500 (epoch 0.04)\n",
      "Minibatch loss: 0.008\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 87.6%\n",
      "Step 600 (epoch 0.04)\n",
      "Minibatch loss: 0.715\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 87.6%\n",
      "Step 700 (epoch 0.05)\n",
      "Minibatch loss: 0.068\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 91.3%\n",
      "Step 800 (epoch 0.06)\n",
      "Minibatch loss: 2.266\n",
      "Minibatch acc:  50.0%\n",
      "Validation error: 87.6%\n",
      "Step 900 (epoch 0.07)\n",
      "Minibatch loss: 0.116\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 92.1%\n",
      "Step 1000 (epoch 0.07)\n",
      "Minibatch loss: 0.456\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 91.9%\n",
      "Step 1100 (epoch 0.08)\n",
      "Minibatch loss: 0.798\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 84.9%\n",
      "Step 1200 (epoch 0.09)\n",
      "Minibatch loss: 0.015\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 93.5%\n",
      "Step 1300 (epoch 0.09)\n",
      "Minibatch loss: 0.312\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 95.1%\n",
      "Step 1400 (epoch 0.10)\n",
      "Minibatch loss: 0.002\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 94.8%\n",
      "Step 1500 (epoch 0.11)\n",
      "Minibatch loss: 0.010\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 89.7%\n",
      "Step 1600 (epoch 0.12)\n",
      "Minibatch loss: 0.012\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.2%\n",
      "Step 1700 (epoch 0.12)\n",
      "Minibatch loss: 0.060\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.9%\n",
      "Step 1800 (epoch 0.13)\n",
      "Minibatch loss: 0.002\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.2%\n",
      "Step 1900 (epoch 0.14)\n",
      "Minibatch loss: 0.376\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 94.4%\n",
      "Step 2000 (epoch 0.15)\n",
      "Minibatch loss: 0.085\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.2%\n",
      "Step 2100 (epoch 0.15)\n",
      "Minibatch loss: 0.002\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.5%\n",
      "Step 2200 (epoch 0.16)\n",
      "Minibatch loss: 0.019\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.7%\n",
      "Step 2300 (epoch 0.17)\n",
      "Minibatch loss: 0.724\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 89.8%\n",
      "Step 2400 (epoch 0.17)\n",
      "Minibatch loss: 0.224\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 94.7%\n",
      "Step 2500 (epoch 0.18)\n",
      "Minibatch loss: 0.044\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 94.8%\n",
      "Step 2600 (epoch 0.19)\n",
      "Minibatch loss: 0.597\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 93.8%\n",
      "Step 2700 (epoch 0.20)\n",
      "Minibatch loss: 0.066\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 93.9%\n",
      "Step 2800 (epoch 0.20)\n",
      "Minibatch loss: 0.075\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.7%\n",
      "Step 2900 (epoch 0.21)\n",
      "Minibatch loss: 0.011\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.1%\n",
      "Step 3000 (epoch 0.22)\n",
      "Minibatch loss: 0.124\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.6%\n",
      "Step 3100 (epoch 0.23)\n",
      "Minibatch loss: 0.388\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 88.9%\n",
      "Step 3200 (epoch 0.23)\n",
      "Minibatch loss: 0.229\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.9%\n",
      "Step 3300 (epoch 0.24)\n",
      "Minibatch loss: 0.042\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 94.5%\n",
      "Step 3400 (epoch 0.25)\n",
      "Minibatch loss: 0.001\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 92.9%\n",
      "Step 3500 (epoch 0.25)\n",
      "Minibatch loss: 0.024\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.8%\n",
      "Step 3600 (epoch 0.26)\n",
      "Minibatch loss: 0.006\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.4%\n",
      "Step 3700 (epoch 0.27)\n",
      "Minibatch loss: 0.049\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.0%\n",
      "Step 3800 (epoch 0.28)\n",
      "Minibatch loss: 0.003\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 92.5%\n",
      "Step 3900 (epoch 0.28)\n",
      "Minibatch loss: 0.032\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.7%\n",
      "Step 4000 (epoch 0.29)\n",
      "Minibatch loss: 0.032\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.2%\n",
      "Step 4100 (epoch 0.30)\n",
      "Minibatch loss: 0.142\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 92.9%\n",
      "Step 4200 (epoch 0.31)\n",
      "Minibatch loss: 0.005\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.9%\n",
      "Step 4300 (epoch 0.31)\n",
      "Minibatch loss: 0.202\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 94.7%\n",
      "Step 4400 (epoch 0.32)\n",
      "Minibatch loss: 0.001\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.2%\n",
      "Step 4500 (epoch 0.33)\n",
      "Minibatch loss: 0.001\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.6%\n",
      "Step 4600 (epoch 0.33)\n",
      "Minibatch loss: 0.049\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 94.7%\n",
      "Step 4700 (epoch 0.34)\n",
      "Minibatch loss: 0.051\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.9%\n",
      "Step 4800 (epoch 0.35)\n",
      "Minibatch loss: 0.007\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.7%\n",
      "Step 4900 (epoch 0.36)\n",
      "Minibatch loss: 0.005\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.7%\n",
      "Step 5000 (epoch 0.36)\n",
      "Minibatch loss: 0.025\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.1%\n",
      "Step 5100 (epoch 0.37)\n",
      "Minibatch loss: 0.052\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.5%\n",
      "Step 5200 (epoch 0.38)\n",
      "Minibatch loss: 0.993\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 90.7%\n",
      "Step 5300 (epoch 0.39)\n",
      "Minibatch loss: 0.004\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.3%\n",
      "Step 5400 (epoch 0.39)\n",
      "Minibatch loss: 0.962\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 91.3%\n",
      "Step 5500 (epoch 0.40)\n",
      "Minibatch loss: 0.005\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.2%\n",
      "Step 5600 (epoch 0.41)\n",
      "Minibatch loss: 0.004\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 93.5%\n",
      "Step 5700 (epoch 0.41)\n",
      "Minibatch loss: 0.011\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.9%\n",
      "Step 5800 (epoch 0.42)\n",
      "Minibatch loss: 0.035\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.5%\n",
      "Step 5900 (epoch 0.43)\n",
      "Minibatch loss: 0.025\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 94.9%\n",
      "Step 6000 (epoch 0.44)\n",
      "Minibatch loss: 0.142\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 92.3%\n",
      "Step 6100 (epoch 0.44)\n",
      "Minibatch loss: 0.001\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.4%\n",
      "Step 6200 (epoch 0.45)\n",
      "Minibatch loss: 0.081\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 93.4%\n",
      "Step 6300 (epoch 0.46)\n",
      "Minibatch loss: 0.056\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.6%\n",
      "Step 6400 (epoch 0.47)\n",
      "Minibatch loss: 0.010\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.0%\n",
      "Step 6500 (epoch 0.47)\n",
      "Minibatch loss: 0.003\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.5%\n",
      "Step 6600 (epoch 0.48)\n",
      "Minibatch loss: 0.009\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.1%\n",
      "Step 6700 (epoch 0.49)\n",
      "Minibatch loss: 0.004\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 94.3%\n",
      "Step 6800 (epoch 0.49)\n",
      "Minibatch loss: 0.035\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 92.8%\n",
      "Step 6900 (epoch 0.50)\n",
      "Minibatch loss: 0.020\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 94.0%\n",
      "Step 7000 (epoch 0.51)\n",
      "Minibatch loss: 0.013\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.6%\n",
      "Step 7100 (epoch 0.52)\n",
      "Minibatch loss: 0.068\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.2%\n",
      "Step 7200 (epoch 0.52)\n",
      "Minibatch loss: 0.004\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 91.1%\n",
      "Step 7300 (epoch 0.53)\n",
      "Minibatch loss: 0.038\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.7%\n",
      "Step 7400 (epoch 0.54)\n",
      "Minibatch loss: 0.002\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 94.5%\n",
      "Step 7500 (epoch 0.55)\n",
      "Minibatch loss: 0.003\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 92.4%\n",
      "Step 7600 (epoch 0.55)\n",
      "Minibatch loss: 0.690\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 95.0%\n",
      "Step 7700 (epoch 0.56)\n",
      "Minibatch loss: 0.014\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.8%\n",
      "Step 7800 (epoch 0.57)\n",
      "Minibatch loss: 0.007\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.4%\n",
      "Step 7900 (epoch 0.57)\n",
      "Minibatch loss: 0.005\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 94.9%\n",
      "Step 8000 (epoch 0.58)\n",
      "Minibatch loss: 0.000\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.9%\n",
      "Step 8100 (epoch 0.59)\n",
      "Minibatch loss: 0.409\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 93.8%\n",
      "Step 8200 (epoch 0.60)\n",
      "Minibatch loss: 0.005\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 94.3%\n",
      "Step 8300 (epoch 0.60)\n",
      "Minibatch loss: 0.002\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.4%\n",
      "Step 8400 (epoch 0.61)\n",
      "Minibatch loss: 0.003\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 92.2%\n",
      "Step 8500 (epoch 0.62)\n",
      "Minibatch loss: 0.048\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.8%\n",
      "Step 8600 (epoch 0.63)\n",
      "Minibatch loss: 0.019\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 94.7%\n",
      "Step 8700 (epoch 0.63)\n",
      "Minibatch loss: 0.038\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.1%\n",
      "Step 8800 (epoch 0.64)\n",
      "Minibatch loss: 0.004\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.5%\n",
      "Step 8900 (epoch 0.65)\n",
      "Minibatch loss: 0.004\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.3%\n",
      "Step 9000 (epoch 0.65)\n",
      "Minibatch loss: 0.099\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.3%\n",
      "Step 9100 (epoch 0.66)\n",
      "Minibatch loss: 0.060\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 90.0%\n",
      "Step 9200 (epoch 0.67)\n",
      "Minibatch loss: 0.301\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 96.4%\n",
      "Step 9300 (epoch 0.68)\n",
      "Minibatch loss: 0.003\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 91.6%\n",
      "Step 9400 (epoch 0.68)\n",
      "Minibatch loss: 0.002\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.4%\n",
      "Step 9500 (epoch 0.69)\n",
      "Minibatch loss: 0.179\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 94.1%\n",
      "Step 9600 (epoch 0.70)\n",
      "Minibatch loss: 0.167\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.2%\n",
      "Step 9700 (epoch 0.71)\n",
      "Minibatch loss: 0.020\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 93.8%\n",
      "Step 9800 (epoch 0.71)\n",
      "Minibatch loss: 0.059\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.7%\n",
      "Step 9900 (epoch 0.72)\n",
      "Minibatch loss: 0.001\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.3%\n",
      "Step 10000 (epoch 0.73)\n",
      "Minibatch loss: 0.040\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.8%\n",
      "Step 10100 (epoch 0.73)\n",
      "Minibatch loss: 0.020\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.4%\n",
      "Step 10200 (epoch 0.74)\n",
      "Minibatch loss: 0.595\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 95.4%\n",
      "Step 10300 (epoch 0.75)\n",
      "Minibatch loss: 0.003\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.8%\n",
      "Step 10400 (epoch 0.76)\n",
      "Minibatch loss: 0.024\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.7%\n",
      "Step 10500 (epoch 0.76)\n",
      "Minibatch loss: 0.001\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 97.0%\n",
      "Step 10600 (epoch 0.77)\n",
      "Minibatch loss: 0.122\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.7%\n",
      "Step 10700 (epoch 0.78)\n",
      "Minibatch loss: 0.001\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.9%\n",
      "Step 10800 (epoch 0.79)\n",
      "Minibatch loss: 0.007\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.7%\n",
      "Step 10900 (epoch 0.79)\n",
      "Minibatch loss: 0.032\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.8%\n",
      "Step 11000 (epoch 0.80)\n",
      "Minibatch loss: 0.844\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 94.4%\n",
      "Step 11100 (epoch 0.81)\n",
      "Minibatch loss: 0.008\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 97.1%\n",
      "Step 11200 (epoch 0.81)\n",
      "Minibatch loss: 0.002\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.8%\n",
      "Step 11300 (epoch 0.82)\n",
      "Minibatch loss: 0.005\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.6%\n",
      "Step 11400 (epoch 0.83)\n",
      "Minibatch loss: 0.001\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 97.1%\n",
      "Step 11500 (epoch 0.84)\n",
      "Minibatch loss: 0.009\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 93.8%\n",
      "Step 11600 (epoch 0.84)\n",
      "Minibatch loss: 0.008\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.7%\n",
      "Step 11700 (epoch 0.85)\n",
      "Minibatch loss: 0.007\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 94.5%\n",
      "Step 11800 (epoch 0.86)\n",
      "Minibatch loss: 0.201\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.7%\n",
      "Step 11900 (epoch 0.87)\n",
      "Minibatch loss: 0.086\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 94.5%\n",
      "Step 12000 (epoch 0.87)\n",
      "Minibatch loss: 0.043\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.0%\n",
      "Step 12100 (epoch 0.88)\n",
      "Minibatch loss: 0.002\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.2%\n",
      "Step 12200 (epoch 0.89)\n",
      "Minibatch loss: 0.058\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.4%\n",
      "Step 12300 (epoch 0.89)\n",
      "Minibatch loss: 0.001\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.4%\n",
      "Step 12400 (epoch 0.90)\n",
      "Minibatch loss: 0.014\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.9%\n",
      "Step 12500 (epoch 0.91)\n",
      "Minibatch loss: 0.001\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.5%\n",
      "Step 12600 (epoch 0.92)\n",
      "Minibatch loss: 0.001\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.3%\n",
      "Step 12700 (epoch 0.92)\n",
      "Minibatch loss: 0.000\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.5%\n",
      "Step 12800 (epoch 0.93)\n",
      "Minibatch loss: 0.003\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.7%\n",
      "Step 12900 (epoch 0.94)\n",
      "Minibatch loss: 0.594\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 90.9%\n",
      "Step 13000 (epoch 0.95)\n",
      "Minibatch loss: 0.004\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.4%\n",
      "Step 13100 (epoch 0.95)\n",
      "Minibatch loss: 0.004\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.2%\n",
      "Step 13200 (epoch 0.96)\n",
      "Minibatch loss: 0.139\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 94.1%\n",
      "Step 13300 (epoch 0.97)\n",
      "Minibatch loss: 0.000\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.1%\n",
      "Step 13400 (epoch 0.97)\n",
      "Minibatch loss: 0.014\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.8%\n",
      "Step 13500 (epoch 0.98)\n",
      "Minibatch loss: 0.000\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 95.8%\n",
      "Step 13600 (epoch 0.99)\n",
      "Minibatch loss: 0.308\n",
      "Minibatch acc:  75.0%\n",
      "Validation error: 96.8%\n",
      "Step 13700 (epoch 1.00)\n",
      "Minibatch loss: 0.003\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 94.8%\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN(num_labels=NUM_LABELS, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE, num_channels=NUM_CHANNELS, seed=SEED)\n",
    "learning_rate = 0.01\n",
    "\n",
    "for step in xrange(int(MAX_EPOCHS * train_size) // BATCH_SIZE):\n",
    "    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "    batch_x = train_data[offset:(offset + BATCH_SIZE), ...]\n",
    "    batch_y = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "\n",
    "    loss, predictions = cnn.fit(batch_x,batch_y,learning_rate)\n",
    "    \n",
    "    if step % EVAL_FREQUENCY == 0:\n",
    "        acc = cal_acc(predictions, np.argmax(batch_y, axis=1))\n",
    "        print('Step %d (epoch %.2f)' % (step, float(step) * BATCH_SIZE / train_size))\n",
    "        print('Minibatch loss: %.3f' % loss)\n",
    "        print('Minibatch acc:  %.1f%%' % acc)\n",
    "        val_predictions = eval_in_batches(validation_data[0:1000, ...],cnn)\n",
    "        val_acc = cal_acc(val_predictions, np.argmax(validation_labels[0:1000], axis=1))\n",
    "        print('Validation error: %.1f%%' % val_acc)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试过程，输出测试集准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data acc: 94.7%\n"
     ]
    }
   ],
   "source": [
    "test_predictions = eval_in_batches(test_data, cnn)\n",
    "test_acc = cal_acc(test_predictions, test_labels)\n",
    "print(\"test data acc: %.1f%%\" % test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_homework1_001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f0063673b51db484840a50d2ac89c01823d9f44f0d663c7f7d10125723239b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
